{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 3176995065613540268\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 3164969369\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 13540953408768866840\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers.core import Activation, Flatten, Dense\n",
    "from keras.layers import BatchNormalization, Dropout\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ('BIG_BAND', 'BLUES_CONTEMPORARY', 'COUNTRY_TRADITIONAL', 'DANCE', 'ELECTRONICA', 'EXPERIMENTAL',\n",
    "              'FOLK_INTERNATIONAL', 'GOSPEL', 'GRUNGE_EMO', 'HIP_HOP_RAP', 'JAZZ_CLASSIC', 'METAL_ALTERNATIVE',\n",
    "              'METAL_DEATH', 'METAL_HEAVY', 'POP_CONTEMPORARY', 'POP_INDIE', 'POP_LATIN', 'PUNK', 'REGGAE',\n",
    "              'RNB_SOUL', 'ROCK_ALTERNATIVE', 'ROCK_COLLEGE', 'ROCK_CONTEMPORARY', 'ROCK_HARD', 'ROCK_NEO_PSYCHEDELIA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives_dataset():\n",
    "    print(\"[INFO] loading derivatives dataset ...\")\n",
    "    derivative_labels = pd.read_csv('tagged_feature_sets/msd-jmirderivatives_dev/msd-jmirderivatives_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(derivative_labels) :\n",
    "        derivative_labels[i] = categories.index(label)\n",
    "    derivative_features = pd.read_csv('tagged_feature_sets/msd-jmirderivatives_dev/msd-jmirderivatives_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(derivative_labels)\n",
    "    np.asarray(derivative_features)\n",
    "    derivative_labels = to_categorical(derivative_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_deriv, X_deriv_test, Y_deriv, Y_deriv_test = train_test_split(derivative_features, derivative_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=derivative_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(derivative_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(derivative_features.shape))\n",
    "    return X_deriv, X_deriv_test, Y_deriv, Y_deriv_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lpc_dataset():\n",
    "    print(\"[INFO] loading lpc dataset ...\")\n",
    "    lpc_labels = pd.read_csv('tagged_feature_sets/msd-jmirlpc_dev/msd-jmirlpc_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(lpc_labels) :\n",
    "        lpc_labels[i] = categories.index(label)\n",
    "    lpc_features = pd.read_csv('tagged_feature_sets/msd-jmirlpc_dev/msd-jmirlpc_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(lpc_labels)\n",
    "    np.asarray(lpc_features)\n",
    "    lpc_labels = to_categorical(lpc_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_lpc, X_lpc_test, Y_lpc, Y_lpc_test = train_test_split(lpc_features, lpc_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=lpc_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(lpc_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(lpc_features.shape))\n",
    "    return X_lpc, X_lpc_test, Y_lpc, Y_lpc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mfccs_dataset():\n",
    "    print(\"[INFO] loading mfccs dataset ...\")\n",
    "    mfccs_labels = pd.read_csv('tagged_feature_sets/msd-jmirmfccs_dev/msd-jmirmfccs_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(mfccs_labels) :\n",
    "        mfccs_labels[i] = categories.index(label)\n",
    "    mfccs_features = pd.read_csv('tagged_feature_sets/msd-jmirmfccs_dev/msd-jmirmfccs_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(mfccs_labels)\n",
    "    np.asarray(mfccs_features)\n",
    "    mfccs_labels = to_categorical(mfccs_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_mfccs, X_mfccs_test, Y_mfccs, Y_mfccs_test = train_test_split(mfccs_features, mfccs_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=mfccs_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(mfccs_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(mfccs_features.shape))\n",
    "    return X_mfccs, X_mfccs_test, Y_mfccs, Y_mfccs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_dataset():\n",
    "    print(\"[INFO] loading moments dataset ...\")\n",
    "    moments_labels = pd.read_csv('tagged_feature_sets/msd-jmirmoments_dev/msd-jmirmoments_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(moments_labels) :\n",
    "        moments_labels[i] = categories.index(label)\n",
    "    moments_features = pd.read_csv('tagged_feature_sets/msd-jmirmoments_dev/msd-jmirmoments_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(moments_labels)\n",
    "    np.asarray(moments_features)\n",
    "    moments_labels = to_categorical(moments_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_moments, X_moments_test, Y_moments, Y_moments_test = train_test_split(moments_features, moments_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=moments_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(moments_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(moments_features.shape))\n",
    "    return X_moments, X_moments_test, Y_moments, Y_moments_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_dataset():\n",
    "    print(\"[INFO] loading spectral dataset ...\")\n",
    "    spectral_labels = pd.read_csv('tagged_feature_sets/msd-jmirspectral_dev/msd-jmirspectral_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(spectral_labels) :\n",
    "        spectral_labels[i] = categories.index(label)\n",
    "    spectral_features = pd.read_csv('tagged_feature_sets/msd-jmirspectral_dev/msd-jmirspectral_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(spectral_labels)\n",
    "    np.asarray(spectral_features)\n",
    "    spectral_labels = to_categorical(spectral_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_spectral, X_spectral_test, Y_spectral, Y_spectral_test = train_test_split(spectral_features, spectral_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=spectral_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(spectral_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(spectral_features.shape))\n",
    "    return X_spectral, X_spectral_test, Y_spectral, Y_spectral_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def marsyas_dataset():\n",
    "    print(\"[INFO] loading marsyas dataset ...\")\n",
    "    marsyas_labels = pd.read_csv('tagged_feature_sets/msd-marsyas_dev_new/msd-marsyas_dev_new.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(marsyas_labels) :\n",
    "        marsyas_labels[i] = categories.index(label)\n",
    "    marsyas_features = pd.read_csv('tagged_feature_sets/msd-marsyas_dev_new/msd-marsyas_dev_new.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(marsyas_labels)\n",
    "    np.asarray(marsyas_features)\n",
    "    marsyas_labels = to_categorical(marsyas_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_marsyas, X_marsyas_test, Y_marsyas, Y_marsyas_test = train_test_split(marsyas_features, marsyas_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=marsyas_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(marsyas_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(marsyas_features.shape))\n",
    "    return X_marsyas, X_marsyas_test, Y_marsyas, Y_marsyas_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mvd_dataset():\n",
    "    print(\"[INFO] loading mvd dataset ...\")\n",
    "    mvd_labels = pd.read_csv('tagged_feature_sets/msd-mvd_dev/msd-mvd_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(mvd_labels) :\n",
    "        mvd_labels[i] = categories.index(label)\n",
    "    mvd_features = pd.read_csv('tagged_feature_sets/msd-mvd_dev/msd-mvd_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(mvd_labels)\n",
    "    np.asarray(mvd_features)\n",
    "    mvd_labels = to_categorical(mvd_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_mvd, X_mvd_test, Y_mvd, Y_mvd_test = train_test_split(mvd_features, mvd_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=mvd_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(mvd_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(mvd_features.shape))\n",
    "    return X_mvd, X_mvd_test, Y_mvd, Y_mvd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rh_dataset():\n",
    "    print(\"[INFO] loading rh dataset ...\")\n",
    "    rh_labels = pd.read_csv('tagged_feature_sets/msd-rh_dev_new/msd-rh_dev_new.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(rh_labels) :\n",
    "        rh_labels[i] = categories.index(label)\n",
    "    rh_features = pd.read_csv('tagged_feature_sets/msd-rh_dev_new/msd-rh_dev_new.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(rh_labels)\n",
    "    np.asarray(rh_features)\n",
    "    rh_labels = to_categorical(rh_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_rh, X_rh_test, Y_rh, Y_rh_test = train_test_split(rh_features, rh_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=rh_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(rh_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(rh_features.shape))\n",
    "    return X_rh, X_rh_test, Y_rh, Y_rh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ssd_dataset():\n",
    "    print(\"[INFO] loading ssd dataset ...\")\n",
    "    ssd_labels = pd.read_csv('tagged_feature_sets/msd-ssd_dev/msd-ssd_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(ssd_labels) :\n",
    "        ssd_labels[i] = categories.index(label)\n",
    "    ssd_features = pd.read_csv('tagged_feature_sets/msd-ssd_dev/msd-ssd_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(ssd_labels)\n",
    "    np.asarray(ssd_features)\n",
    "    ssd_labels = to_categorical(ssd_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_ssd, X_ssd_test, Y_ssd, Y_ssd_test = train_test_split(ssd_features, ssd_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=ssd_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(ssd_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(ssd_features.shape))\n",
    "    return X_ssd, X_ssd_test, Y_ssd, Y_ssd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trh_dataset():\n",
    "    print(\"[INFO] loading trh dataset ...\")\n",
    "    trh_labels = pd.read_csv('tagged_feature_sets/msd-trh_dev/msd-trh_dev.csv',\n",
    "                                    delimiter=',', header=None).values[:, -1:]\n",
    "    for i, label in enumerate(trh_labels) :\n",
    "        trh_labels[i] = categories.index(label)\n",
    "    trh_features = pd.read_csv('tagged_feature_sets/msd-trh_dev/msd-trh_dev.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    np.asarray(trh_labels)\n",
    "    np.asarray(trh_features)\n",
    "    trh_labels = to_categorical(trh_labels, num_classes=25)\n",
    "    print(\"[INFO] splitting the data ...\")\n",
    "    X_trh, X_trh_test, Y_trh, Y_trh_test = train_test_split(trh_features, trh_labels,\n",
    "                                                                    test_size=0.20, random_state=42,\n",
    "                                                                    stratify=trh_labels)\n",
    "    print(\"[INFO] Labels format : {0}\".format(trh_labels.shape))\n",
    "    print(\"[INFO] Features format : {0}\".format(trh_features.shape))\n",
    "    return X_trh, X_trh_test, Y_trh, Y_trh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deepMLP(num_features, num_classes):\n",
    "    n_hidden_1 = 100 # 1st layer number of neurons\n",
    "    n_hidden_2 = 75 # 2nd layer number of neurons\n",
    "    n_hidden_3 = 50 # 3rd layer number of neurons\n",
    "    n_hidden_4 = 30 # 3rd layer number of neurons\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden_1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(n_hidden_2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(n_hidden_3, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(n_hidden_4, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "def wideMLP(num_features, num_classes):\n",
    "    n_hidden_1 = 200 # 1st layer number of neurons\n",
    "    n_hidden_2 = 75  # 2nd layer number of neurons\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(n_hidden_1, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(n_hidden_2, activation='relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = len(categories)\n",
    "num_features = 0\n",
    "BATCH_SIZE = 500\n",
    "INIT_LR = 5e-4\n",
    "EPOCHS = 50\n",
    "Datasets = ['derivatives', 'lpc', 'mfccs', 'moments', 'spectral', 'marsyas', 'mvd', 'rh', 'ssd', 'trh']\n",
    "deep_val_acc = []\n",
    "wide_val_acc = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaleDataset(X, X_test, num_features):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    if X_test!=None :\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "    num_features = X.shape[1]\n",
    "    return X, X_test, num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDeepModel(num_classes, num_features, X, X_test, Y, Y_test, EPOCHS, BATCH_SIZE, INIT_LR, chosenDataset):\n",
    "    plots = False\n",
    "    deepModel = deepMLP(num_features, num_classes)\n",
    "    deepModel.compile(optimizer=Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"[INFO] training deep model on {0} dataset\".format(chosenDataset))\n",
    "    deepH = deepModel.fit(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                          validation_data=(X_test, Y_test), verbose = 0)\n",
    "    deepModel.save(\"ModelsMLP\\\\\"+\"deepMPL\"+chosenDataset+\".hdf5\")\n",
    "    \n",
    "    if plots :\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(0, EPOCHS), deepH.history[\"loss\"], label=\"train_loss\")\n",
    "        plt.plot(np.arange(0, EPOCHS), deepH.history[\"val_loss\"], label=\"val_loss\")\n",
    "        plt.plot(np.arange(0, EPOCHS), deepH.history[\"acc\"], label=\"train_acc\")\n",
    "        plt.plot(np.arange(0, EPOCHS), deepH.history[\"val_acc\"], label=\"val_acc\")\n",
    "        plt.title(\"Training Loss and Accuracy on \"+chosenDataset)\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(\"GraphsMLP\\\\\"+\"deepMPL\"+chosenDataset+\".png\")\n",
    "    return deepH.history[\"val_acc\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createWideModel(num_classes, num_features, X, X_test, Y, Y_test, EPOCHS, BATCH_SIZE, INIT_LR, chosenDataset):\n",
    "    plots = False\n",
    "    wideModel = wideMLP(num_features, num_classes)\n",
    "    wideModel.compile(optimizer=Adam(lr=INIT_LR, decay=INIT_LR / EPOCHS),\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    print(\"[INFO] training wide model on {0} dataset\".format(chosenDataset))\n",
    "    wideH = wideModel.fit(X, Y, epochs=EPOCHS, batch_size=BATCH_SIZE, \n",
    "                          validation_data=(X_test, Y_test), verbose = 0)\n",
    "    wideModel.save(\"ModelsMLP\\\\\"+\"wideMPL\"+chosenDataset+\".hdf5\")\n",
    "\n",
    "    if plots :\n",
    "        plt.style.use(\"ggplot\")\n",
    "        plt.figure()\n",
    "        plt.plot(np.arange(0, EPOCHS), wideH.history[\"loss\"], label=\"train_loss\")\n",
    "        plt.plot(np.arange(0, EPOCHS), wideH.history[\"val_loss\"], label=\"val_loss\")\n",
    "        plt.plot(np.arange(0, EPOCHS), wideH.history[\"acc\"], label=\"train_acc\")\n",
    "        plt.plot(np.arange(0, EPOCHS), wideH.history[\"val_acc\"], label=\"val_acc\")\n",
    "        plt.title(\"Training Loss and Accuracy on \"+chosenDataset)\n",
    "        plt.xlabel(\"Epoch #\")\n",
    "        plt.ylabel(\"Loss/Accuracy\")\n",
    "        plt.legend(loc=\"lower left\")\n",
    "        plt.savefig(\"GraphsMLP\\\\\"+\"wideMPL\"+chosenDataset+\".png\")\n",
    "    return wideH.history[\"val_acc\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading derivatives dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 96)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on derivatives dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on derivatives dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading lpc dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 20)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on lpc dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on lpc dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading mfccs dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 26)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on mfccs dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on mfccs dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading moments dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 10)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on moments dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on moments dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading spectral dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on spectral dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on spectral dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading marsyas dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 124)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on marsyas dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on marsyas dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading mvd dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 420)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on mvd dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on mvd dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading rh dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 60)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on rh dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on rh dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading ssd dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 168)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on ssd dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on ssd dataset\n",
      "[INFO] wide model : last validation accuracy : \n",
      "[INFO] loading trh dataset ...\n",
      "[INFO] splitting the data ...\n",
      "[INFO] Labels format : (179555, 25)\n",
      "[INFO] Features format : (179555, 420)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "C:\\Users\\hugol\\AppData\\Local\\conda\\conda\\envs\\GTI770\\lib\\site-packages\\sklearn\\utils\\validation.py:590: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training deep model on trh dataset\n",
      "[INFO] deep model : last validation accuracy : \n",
      "[INFO] training wide model on trh dataset\n",
      "[INFO] wide model : last validation accuracy : \n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Datasets)):\n",
    "    chosenDataset = Datasets[i]\n",
    "    X, X_test, Y, Y_test = locals()[chosenDataset+'_dataset']()\n",
    "    X, X_test, num_features = scaleDataset(X, X_test, num_features)\n",
    "    deep_val_acc.append(createDeepModel(num_classes, num_features, X, X_test, Y, Y_test, \n",
    "                                        EPOCHS, BATCH_SIZE, INIT_LR, chosenDataset))\n",
    "    print(\"[INFO] deep model : last validation accuracy : \".format(deep_val_acc[-1]))\n",
    "    wide_val_acc.append(createWideModel(num_classes, num_features, X, X_test, Y, Y_test, \n",
    "                                        EPOCHS, BATCH_SIZE, INIT_LR, chosenDataset))\n",
    "    print(\"[INFO] wide model : last validation accuracy : \".format(wide_val_acc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.26844142548044253, 0.21291526274573072, 0.25309793664963204, 0.20314109944634323, 0.23246358975986767, 0.3019687554250434, 0.2336053012492832, 0.20673331418055607, 0.29375400356586334, 0.21578346411596813]\n",
      "[0.2838963000272663, 0.22229957385375343, 0.2636518068674901, 0.20848765048135615, 0.24315669390919287, 0.3193450466793006, 0.23995433250288462, 0.21547715290739788, 0.3089861052731604, 0.21700871601098695]\n"
     ]
    }
   ],
   "source": [
    "print(deep_val_acc)\n",
    "print(wide_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best scores with deep MLP on :\n",
      "marsyas with 0.3019687554250434\n",
      "ssd with 0.29375400356586334\n",
      "derivatives with 0.26844142548044253\n",
      "\n",
      "Best scores with wide MLP on :\n",
      "marsyas with 0.3193450466793006\n",
      "ssd with 0.3089861052731604\n",
      "derivatives with 0.2838963000272663\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from heapq import nlargest\n",
    "bests = 3\n",
    "deep_max_scores_indices = []\n",
    "wide_max_scores_indices = []\n",
    "deep_val_acc_temp = [0.26844142548044253, 0.21291526274573072, 0.25309793664963204, 0.20314109944634323, 0.23246358975986767, \n",
    "                     0.3019687554250434, 0.2336053012492832, 0.20673331418055607, 0.29375400356586334, 0.21578346411596813]\n",
    "wide_val_acc_temp = [0.2838963000272663, 0.22229957385375343, 0.2636518068674901, 0.20848765048135615, 0.24315669390919287, \n",
    "                     0.3193450466793006, 0.23995433250288462, 0.21547715290739788, 0.3089861052731604, 0.21700871601098695]\n",
    "deep_max_scores = nlargest(bests, deep_val_acc_temp)\n",
    "wide_max_scores = nlargest(bests, wide_val_acc_temp)\n",
    "\n",
    "for i in range(bests):\n",
    "    deep_max_scores_indices.append(deep_val_acc_temp.index(deep_max_scores[i]))\n",
    "    wide_max_scores_indices.append(wide_val_acc_temp.index(wide_max_scores[i]))\n",
    "\n",
    "print(\"Best scores with deep MLP on :\\n{0} with {1}\\n{2} with {3}\\n{4} with {5}\\n\".format(Datasets[deep_max_scores_indices[0]], \n",
    "                                                                                        deep_max_scores[0], \n",
    "                                                                                        Datasets[deep_max_scores_indices[1]], \n",
    "                                                                                        deep_max_scores[1], \n",
    "                                                                                        Datasets[deep_max_scores_indices[2]], \n",
    "                                                                                        deep_max_scores[2]))\n",
    "print(\"Best scores with wide MLP on :\\n{0} with {1}\\n{2} with {3}\\n{4} with {5}\\n\".format(Datasets[wide_max_scores_indices[0]], \n",
    "                                                                                        wide_max_scores[0], \n",
    "                                                                                        Datasets[wide_max_scores_indices[1]], \n",
    "                                                                                        wide_max_scores[1], \n",
    "                                                                                        Datasets[wide_max_scores_indices[2]], \n",
    "                                                                                        wide_max_scores[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelsMLP\\wideMPLmarsyas.hdf5\n"
     ]
    }
   ],
   "source": [
    "target_model = ['wide', 5]\n",
    "model_name = 'ModelsMLP\\\\'+target_model[0]+'MPL'+Datasets[target_model[1]]+'.hdf5'\n",
    "print(model_name)\n",
    "model = load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading untagged marsyas dataset ...\n",
      "[INFO] Features format : (92499, 124)\n",
      "[INFO] IDs format : (92499,)\n"
     ]
    }
   ],
   "source": [
    "def untagged_dataset(dataset):\n",
    "    print(\"[INFO] loading untagged {0} dataset ...\".format(dataset))\n",
    "    if dataset in ('derivatives', 'lpc', 'mfccs', 'moments', 'spectral'):\n",
    "        dataset = 'jmir'+dataset\n",
    "    if dataset == 'marsyas' :\n",
    "        new = 'new_'\n",
    "    else :\n",
    "        new = ''\n",
    "    features = pd.read_csv('untagged_feature_sets/msd-'+dataset+'_test_'+new+'nolabels/msd-'+dataset+'_test_'+new+'nolabels.csv',\n",
    "                                      delimiter=',', header=None).values[:, 2:-1]\n",
    "    IDs = pd.read_csv('untagged_feature_sets/msd-'+dataset+'_test_'+new+'nolabels/msd-'+dataset+'_test_'+new+'nolabels.csv',\n",
    "                                      delimiter=',', header=None).values[:, 1]\n",
    "    np.asarray(features)\n",
    "    np.asarray(IDs)\n",
    "    print(\"[INFO] Features format : {0}\".format(features.shape))\n",
    "    print(\"[INFO] IDs format : {0}\".format(IDs.shape))\n",
    "    return IDs, features\n",
    "\n",
    "IDs, guess_features = untagged_dataset(Datasets[target_model[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_labels = model.predict(guess_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess_categories = []\n",
    "for i in range(len(guess_labels)):\n",
    "    guess_categories.append(categories[np.argmax(guess_labels[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('result.csv', 'w', newline='') as csvfile:\n",
    "    filewriter = csv.writer(csvfile, delimiter=',')\n",
    "    filewriter.writerow(['id', 'genre'])\n",
    "    for i in range(len(IDs)):\n",
    "        filewriter.writerow([IDs[i], guess_categories[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
